 "AQUAMUSE: Automatically Generating Datasets for Query-Based Multi-Document Summarization ",
 "arXiv:2010.12694v1  [cs.CL]  23 Oct 2020 ",
 "Sayali Kulkarni ",
 "Sheide Chammas ",
 "Wan Zhu ",
 "Google Research ",
 "Google Research ",
 "Google Research ",
 "sayali@google.com ",
 "sheide@google.com ",
 "wanzhu@google.com ",
 "Fei Sha ",
 "Google Research ",
 "fsha@google.com ",
 "Eugene Ie ",
 "Google Research ",
 "eugeneie@google.com ",
 "Abstract ",
 "Summarization is the task of compressing source document(s) into coherent and succinct passages. This is a valuable tool to present users with concise and accurate sketch of the top ranked documents related to their queries. Query-based multi-document summarization (qMDS) addresses this pervasive need, but the research is severely limited due to lack of training and evaluation datasets as existing single-document and multi-document summarization datasets are inadequate in form and scale. We propose a scalable approach called AQUAMUSE to automatically mine qMDS examples from question answering datasets and large document corpora. Our approach is unique in the sense that it can general a dual dataset \u2014 for extractive and abstractive summaries both. We publicly release a specific instance of an AQUAMUSE dataset with 5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl1 (<>). Extensive evaluation of the dataset along with baseline summarization model experiments are provided. ",
 "1 Introduction ",
 "Summarization has been a challenging problem in natural language processing. Recently a number of neural encoder-decoder approaches have made significant progress in this research area ((<>)Rush (<>)et al., (<>)2015; (<>)See et al., (<>)2017; (<>)Wang et al., (<>)2018). State-of-the-art models such as PEGASUS ((<>)Zhang (<>)et al., (<>)2019) have leveraged related data sources and tasks, such as language modeling, to pre-train massive summarization models. But only a few large-scale high-quality human-curated summarization datasets available for training and evaluation. ",
 "Obtaining human annotation for summarization is a nontrivial task in itself and there are several con-",
 "tributing factors. Summarization is often subjective and depends on the annotators\u2019 reading comprehension abilities (especially on unfamiliar topics), their interpretation of the text and their judgement on what piece of information should be considered important or relevant to the use case of the generated summaries. These are all influenced by the annotators\u2019 own life experiences, and in the case of abstractive summarization, their ability to compose fluent and succinct text passages as well. In some scenarios, such as generating news headlines or identifying how-to instructions (as a form of summary outline), it is possible to obtain and repur-pose pre-annotated data from established web publishers for single document summarization (SDS) ((<>)Hermann et al., (<>)2015a; (<>)Koupaee and Wang, (<>)2018). However it\u2019s less clear how one would approach data collection for multi-document summarization (MDS) from the open web. ",
 "1(<https://commoncrawl.org>)https://commoncrawl.org ",
 "Two recent studies, WikiSum and Multi-News, have attempted to tackle this problem with automatic procedures to harvest documents for MDS by crawling hyperlinks from Wikipedia and newser.com web sites respectively ((<>)Liu et al., (<>)2018; (<>)Fabbri et al., (<>)2019). Both studies target significantly longer summaries than short texts or headlines. However, the area of generating focused summaries conditioned on contexts has not been in the limelight. This is an important problem in natural language generation, for example in personalized news feed summaries, context-driven product review summaries, to name a few. ",
 "Our work considers this variant of MDS called query-based MDS (qMDS) which have crucial applications in augmenting information retrieval (IR) experiences ((<>)Daume\u00b4 III and Marcu, (<>)2006; (<>)Litvak (<>)and Vanetik, (<>)2017; (<>)Hasselqvist et al., (<>)2017; (<>)Baumel (<>)et al., (<>)2018a). Text documents are typically multifaceted and users are often interested in identifying information that is most relevant to their stated pref-",
 "Figure 1: AQUAMUSE pipeline for generating conjugate abstractive and extractive query based multi-document summarization datasets. ",
 "erences. For example, suppose a user is interested in car reliability and cars from certain manufacturers, then an effective IR system could consolidate car reviews from across the web and provide concise summaries relating to the reliability of those cars of interest. Similar to MDS, qMDS also suffers from the lack of large-scale annotated data, especially for generating long abstractive summaries ((<>)Nema et al., (<>)2017). While we mainly focus on qMDS, the proposed dataset generation methodology can be re-adapted for the more general problem of MDS (while the reverse is not necessarily true). ",
 "Our contributions in this paper are two-folds. We first introduce a general approach for machine generating query-based multi-document summarization (qMDS) datasets at scale, with knobs to control the automatically generated outputs along dimensions such as document diversity and degree of relevance of the target summary. Second, we provide an automatically generated large-scale dataset for qMDS that we validate with baseline summarization experiments and human rater evaluations. ",
 "As aforementioned, we focus on summarizing several documents as multi-faceted answers to complex queries. To this end, we leverage the publicly released Google Natural Questions (NQ) dataset ((<>)Kwiatkowski et al., (<>)2019), which contains real user queries from Google search logs, capturing a wide range of topics that interest people. Many questions have short answers (e.g., one or more entity names, or dates) derived from Wikipedia pages and have been used to form NQ question answering dataset for training a SQuAD-like span-based QA system. More importantly, a sizable portion of the questions are paired with long-form answers (e.g., paragraphs) that are vetted by human raters. These long-form answers address user questions ",
 "with content that are focused and coherent. As the Wikipedia passages have also been read and edited by the Wikipedia readership, the writing of the passages should be of adequate quality. ",
 "As we are primarily interested in the qMDS setting for general web IR applications, we would like to simulate how a search engine might synthesize documents of high relevance to a user query. To identify high quality passages from the web that can be used to recreate target Wikipedia paragraphs, we use a pre-processed and cleaned version of the Common Crawl corpus ((<>)Raffel et al., (<>)2019) as a proxy web search index to select documents relevant to the NQ long-form answers. We take special considerations in including documents of varying semantic relevance such that our baseline task involves deriving summaries from documents with enough distracting information to challenge summarization models. Furthermore we ensure the sources are sufficiently diverse among themselves, as our primary interest is to summarize multi-faceted information. Figure (<>)1 illustrates the overall data generation procedure. We publicly release an instance of such a dataset containing 5,519 qMDS examples, that we split into training, validation and tests sets of sizes 4,555, 440 and 524 respectively2 (<>). Each example contains an average of 6 source documents to be synthesized into a long-form answer. ",
 "It is worth noting that the approach used in (<>)Liu (<>)et al. ((<>)2018) to harvest Wikipedia article texts as summary targets is related to ours but there are a few key distinctions in goal and methodology. In the foremost, we aim to provide high-quality dataset for the task of qMDS (not just MDS). As ",
 "2(<https://github.com/google-research-datasets/aquamuse>)https://github.com/ (<https://github.com/google-research-datasets/aquamuse>)google-research-datasets/aquamuse ",
 "such, we aim to generate paragraphs that are more consistent and coherent than generating full length articles, which have much stronger variability in both structure and content. The use of cited references in the WikiSum dataset do not necessarily provide adequate coverage for the summaries especially if some sentences in the Wikipedia text are missing references. Instead we use crawled documents from the web as potential source material. Since these are \u201cnaturally occurring\u201d documents from the web (albeit a cached subset), we are simulating a realistic web IR application scenario across a large document corpus. ",
 "2 AQUAMUSE ",
 "The qMDS problem is formalized as follows. Given a query q, a set of related documents R = {ri}, document passages {ri,j} relevant to the query are synthesized to an answer a. Various MDS approaches synthesize a such that it is succinct and fluent natural language text that covers the information content in {ri,j} rather than just a concatenation of relevant spans. Such synthesized answers can augment information retrieval (IR) applications by enhancing the user experience with high-level query specific summaries. ",
 "We propose an automated approach to generating large datasets for the qMDS task for training and evaluating both abstractive and extractive approaches. We illustrate our approach using Google\u2019s Natural Questions (NQ) and Common Crawl (CC). But the methodology is general enough to be extended to any other question answering dataset (containing answers that span multiple sentences) and web corpora (to serve as the domain for retrieval). ",
 "Google\u2019s NQ is an open-domain question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples in version 1.0((<>)Kwiatkowski et al., (<>)2019). Each example is a Google search query (generated by real users) paired with a crowd sourced short answer (one or more entities) and/or long answer span (typically a paragraph) from a Wikipedia page. Queries annotated with only a long answer span serve as summarization targets since these cannot be addressed tersely by entity names (e.g. \u201cWho lives in the Imperial Palace in Tokyo?\u201d) or a boolean. These queries result in open-ended and complex topics answers (e.g., \u201cWhat does the word China mean in Chinese?\u201d). ",
 "2.1 Approach ",
 "Suppose a long answer is comprised of n sentences a = [l1, ..., ln] and a document corpus D = {di} consists of sentences [di,j]. We use the Universal Sentence Encoder ((<>)Cer et al., (<>)2018) (\u03c6) to encode sentences \u03c6(lk) and \u03c6(di,j) for semantic similarity comparisons (e.g., using a dot product sk,i,j = \ue068\u03c6(lk), \u03c6(di,j)\ue069). This yield in partial result sets Rk = {(di, sk,i,j) : \u03b8U > sk,i,j > \u03b8L}. \ue050 These sets R1..n are then combined by \u03c8(di) = k,j sk,i,j to yield document-level scores to get result set R = {(di, \u03c8(di))}. We restrict the result set by selecting the top-K ranked documents. While we have made specific choices for \u03c6, sk,i,j, \u03c8, they can be customized and tuned to construct result sets R with higher/lower diversity, tighter/looser topicality match, or number of documents retrieved (to name a few) for evaluating summarization approaches under different qMDS task conditions. ",
 "With appropriate tuning of \u03b8U and \u03b8L, the process above admits documents with sentences of varying semantic relevance into the result set R. Lowering \u03b8U (while keeping \u03b8L high enough) ensures we don\u2019t retain sentences di,j that are exact matches of lk (but of at least some semantic equivalence), thereby generating qMDS abstractive summarization examples (q, a, R). The relationship between q and R is transitive through the annotated long answer span a. For constructing extractive qMDS examples, we perform an in-place substitution of di,j with lk (that can optionally be sampled according to match score sk,i,j in future work). ",
 "2.2 Implementation details ",
 "We use a pre-processed and cleaned version of the English CC corpus called the Colossal Clean Crawled Corpus ((<>)Raffel et al., (<>)2019). It contains 355M web pages in total. For the question answering data source, we use a 62.5% sample of the NQ dataset from the train and development splits, in which 8.2% are question answering examples that we matched with the CC corpus. These NQ questions are marked \u201cgood\u201d by a majority of NQ raters and are paired with long-form answers. We limit to question answering pairs that cannot be addressed by terse responses (e.g. factoids), to simulate realistic qMDS use cases. ",
 "Using TensorFlow Hub3 (<>)we compute Universal Sentence Embeddings (which are approximately normalized) for sentences tokenized from both NQ ",
 "3(<https://tfhub.dev/>)https://tfhub.dev/ ",
 "Dataset ",
 "# queries ",
 "# examples ",
 "summary ",
 "inputs ",
 "per-input doc ",
 "train ",
 "dev ",
 "test ",
 "# words ",
 "# sents ",
 "# words ",
 "# sents ",
 "# words ",
 "# sents ",
 "AQUAMUSE ",
 "5,519 ",
 "4,555 ",
 "440 ",
 "524 ",
 "105.9 ",
 "3.8 ",
 "9,764.1 ",
 "405.7 ",
 "1,597.1 ",
 "66.4 ",
 "Debatepedia ",
 "13,719 ",
 "12,000 ",
 "719 ",
 "1,000 ",
 "11.2 ",
 "1 ",
 "75.1 ",
 "5.1 ",
 "75 ",
 "5.1 ",
 "Multi-News ",
 "NA ",
 "44,972 ",
 "5,622 ",
 "5,622 ",
 "263.7 ",
 "10.0 ",
 "2,103.5 ",
 "82.7 ",
 "489.2 ",
 "23.4 ",
 "CNN/DM ",
 "NA ",
 "287,227 ",
 "13,368 ",
 "11,490 ",
 "56.2 ",
 "3.7 ",
 "810.6 ",
 "39.8 ",
 "NA ",
 "NA ",
 "Table 1: Comparison of recently proposed summarization datasets. ",
 "Figure 2: Coverage versus normalized density plot for the abstractive dataset shows the variance in the summary compared to inputs. The high compression ratio coming from long input documents would create a unique challenge for qMDS models. ",
 "and CC data sources. The encoded CC sentences are around 11Tb on disk while the NQ portion that formulates the target summaries is comparatively negligible in size. An exhaustive all pairwise comparison is performed using Apache Beam4 (<>). The sentences from the NQ long answers are matched with the CC corpus using efficient nearest neighbor searches over sentence embeddings indexed by space partitioning trees ((<>)Liu et al., (<>)2004). ",
 "Sentence Matching Thresholds \u03b8U and \u03b8L control the semantic relevance of sentences matched between CC and NQ. Sentence pairs with matching scores below 0.8 are filtered out (\u03b8L). To avoid exact sentence matches from pages with near-Wikipedia duplicates, we also filter out sentence pairs with scores above 0.99 (\u03b8U ). The CC document match score is based on the sum of these sentence-to-sentence match scores. This can be used to trade-off the quality of the matched documents and the abstractive nature of the task. We use the coverage and density metrics defined in (<>)Grusky (<>)et al. ((<>)2018) to construct the normalized bivariate density plot illustrated in Figure (<>)2. ",
 "4(<https://beam.apache.org/>)https://beam.apache.org/ ",
 "# input docs ",
 "1 ",
 "2 ",
 "3 ",
 "4 ",
 "5 ",
 "6 ",
 "7 ",
 "# queries ",
 "198 ",
 "248 ",
 "270 ",
 "258 ",
 "221 ",
 "229 ",
 "4,095 ",
 "Table 2: Distribution of the dataset with different number of documents in the input. ",
 "Summary Recall It is entirely possible that we cannot locate a match for every sentence in a long answer. The Summary Recall is the fraction of sentences in a long answer with matching CC document sentences. A summary recall of 1.0 guarantees a summary can be generated. In our specific dataset instance, we restrict the summary recall to 0.75. Though this may seem like a handicap, we observed in experiments that the input documents have enough information to reconstruct the target summary fully and can be used to test the language generation capabilities of qMDS approaches. ",
 "Top-K Parameter This is analogous to the number of top results returned by web search and controls the degree of support as well as diversity across multiple documents for a qMDS task. We evaluated the quality of the matched documents as ranked by their match scores. We use K = 7 as we found document quality degrade after that (given our specific settings of sentence matching threshold and summary recall). ",
 "2.3 Dataset Statistics ",
 "Our specific dataset instance is derived from a subset of the NQ dataset that we match with CC. Based on the thresholds detailed above, we construct 5,519 examples that are split into training, development and testing sets (4,555, 440 and 524 examples, respectively) using a hash of the NQ long answer. Given the thresholds \u03b8 and K, the total number of CC documents that matched our restricted set of NQ long answers is 33,760. Not every example can find up to K = 7 matching documents from CC within the \u03b8 bounds. The distribution of input document count are as shown in Table (<>)2. ",
 "Query Types Although we explicitly pick the examples from NQ which only have a long answer, ",
 "Figure 3: Overlap between the summary and input documents. 30% of examples have bigram overlap score greater than 0.9 indicating novel bigrams in summary for over 70% of cases. ",
 "we find many descriptive queries for factoid-like questions. For example, \u201cWhere is silver found and in what form (compound)\u201d, \u201cWhere was Moses when he saw a burning bush\u201d, \u201cWhen is a jury used in civil cases\u201d. The most interesting examples are the why queries since they have a descriptive summary in the long-form answer. For example, \u201cWhy does Friedman think the world is flat\u201d, \u201cWhy do plants drip water from their leaves\u201d. ",
 "Document and Summary Lexical Overlaps Our approach relies on sentence-level matching to retrieve documents, but that does not guarantee a high recall of the n-grams in the summary. To sanity check that the set of no more than 7 documents retrieved this way has a high lexical overlap with the summary, we used the BLEU precision score. Note that a perfect BLEU precision score in this case implies that every n-gram in the summary can be mapped to a distinct n-gram in the source. Figure (<>)3 shows the histogram of this overlap measure. ",
 "Comparing to other datasets Table (<>)1 compares our dataset instance with other commonly used datasets for summarization. CNN/DM is an abstractive SDS dataset ((<>)Nallapati et al., (<>)2016). Multi-News is the first large-scale MDS dataset ((<>)Fabbri (<>)et al., (<>)2019). While Multi-News has more summarization examples, our dataset includes query contexts and covers more documents, sentences and tokens per summary. Also the number AQUAMUSE examples can increase with looser restrictions on \u03b8 and K. Furthermore, our approach generalizes to more MDS examples if we removed the query context and operated on any Wikipedia paragraph spans. Recently, (<>)Nema et al. ((<>)2017) introduced a qMDS dataset built from Debatepedia5 (<>). Their input documents are relatively short (75 ",
 "5(<http://www.debatepedia.org>)http://www.debatepedia.org ",
 "Figure 4: Majority decisions for sentence-to-document relevance task across examples rated where 1 sentence was sampled from each 7 matched CC document. Examples are sorted by count of +1 majority decision. Less relevant examples contain more raters abstaining from making a definitive decision. ",
 "words/doc). AQUAMUSE includes much longer input documents that can be more challenging for qMDS models. ",
 "3 Quality Assessment ",
 "In this section, we carefully assess the quality of the automatically generated qMDS examples along several axes: correctness of matched documents, fluency of machine edited extractive summaries, and overall example quality. All our human evaluation tasks are based on human rater pools consisting of fluent English speakers. The annotation tasks are discriminative in nature (e.g., judging semantic match), which are cheaper to source and easier to validate through replication than generative annotation tasks (e.g., open-ended text generation). We also provide a few qMDS examples for illustration. ",
 "3.1 Correctness ",
 "We first evaluate the factual and semantic correctness of the CC documents that were matched with the NQ long answers. We focus on the abstractive setup as we will demonstrate later how the derived extractive version is qualitatively similar. ",
 "For this annotation task, we presented raters with a Wikipedia paragraph (corresponding to the long-form answer) and a matched sentence (one from each of the top-7 CC documents). They were asked to rate \u201c+1\u201d if the CC sentence matched some of the content of the Wikipedia paragraph. Raters were instructed not to rely on external knowledge in the rating process. Numerical facts were subjectively evaluated, e.g., 4B years is close to 4.5B years, but 3 electrons and 4 electrons is not. ",
 "We rated a sample of 5,215 examples corresponding to 856 queries. Each example rating is replicated 3 times across different raters to account for subjectivity. Raters were allowed to abstain if they cannot make a decision. We found that 85.18% of ",
 "Figure 5: Higher ranked documents contain more sentences marked +1 by majority. This is expected as high scoring documents should correlate with higher semantic relevance to the long answer. ",
 "the examples are marked relevant by majority as illustrated in Figure (<>)4. Sentences from top ranked documents (per document match scores) contains many more sentences annotated with +1 majority decision as shown in Figure (<>)5. ",
 "3.2 Fluency ",
 "The extractive dataset is created by replacing sentences from the CC doc with the matched sentence in Wikipedia long answer a. This, however, may distort the overall flow of the original CC passage. This evaluation task ensures that the fluency is not harmful. ",
 "First, we designed a human evaluation where the raters were presented with the original and the edited CC document passages including the replaced sentence. A +1 marks the replaced sentence does not appear out of place. We rated 500 examples with rater replication of 3. In 96.20% examples, these were rated positive. ",
 "Second, we measured the perplexity of the paragraphs that with replaced sentences using a language model 6 (<>). The mean perplexity increased slightly from 80 to 82 after replacement. This small increase is expected since a foreign sentence was inserted, but the difference is small enough proving that the fluency is preserved. ",
 "3.3 Overall quality ",
 "We now turn to evaluating the overall quality of a random sample of 55 qMDS example triplets (q, R, a) along three dimensions \u2014 referential clarity, focus and the coherence of the summary \u2014 adapted from DUC2007 task7 (<>). Since the summary ",
 "6(<https://tfhub.dev/google/wiki40b-lm-en/1>)https://tfhub.dev/google/ (<https://tfhub.dev/google/wiki40b-lm-en/1>)wiki40b-lm-en/1 ",
 "7(<https://duc.nist.gov/duc2007/quality-questions.txt>)https://duc.nist.gov/duc2007/ (<https://duc.nist.gov/duc2007/quality-questions.txt>)quality-questions.txt ",
 "5 ",
 "4 ",
 "3 ",
 "2 ",
 "1 ",
 "NA ",
 "Clarity ",
 "78 ",
 "5 ",
 "2 ",
 "4 ",
 "2 ",
 "9 ",
 "Focus ",
 "67 ",
 "13 ",
 "4 ",
 "0 ",
 "0 ",
 "16 ",
 "Coherence ",
 "71 ",
 "11 ",
 "0 ",
 "4 ",
 "2 ",
 "13 ",
 "Table 3: Distribution of clarity, focus and coherence ratings of abstractive qMDS examples on a 5-point scale. The scale runs from 5 (very good) to 1 (very poor), with NA designating inter-rater disagreements. ",
 "Query: what is a dream and why do we dream ",
 "Source1: Dreams are successions of images, ideas, emotions and sensations occurring involuntarily in the mind during certain stages of sleep. The content and purpose of dreams are not yet understood, though they have been a topic of speculation and interest throughout recorded history. The scientific study of dreams is known as oneirology... ",
 "Source2: A dream is a succession of images ideas emotions and sensations that usually occur involuntarily in the mind during certain stages of sleep. The content and purpose of dreams are not fully understood although they have been a topic of scientific philosophical and religious interest throughout recorded history. ... ",
 "Summary: A dream is a succession of images, ideas, emotions, and sensations that usually occur involuntarily in the mind during certain stages of sleep. The content and purpose of dreams are not fully understood, though they have been a topic of scientific speculation, as well as a subject of philosophical and religious interest, throughout recorded history. Dream interpretation is the attempt at drawing meaning from dreams and searching for an underlying message. The scientific study of dreams is called oneirology. ",
 "Table 4: Multi-faceted query with summary generated using content merged from multiple sources. ",
 "a is a Wikipedia passage, grammatical correctness and redundancy dimensions need not be evaluated. ",
 "Each triplet was rated by 3 raters. The raters were also instructed to consider the query q when evaluating the focus of the summary a rather than just a generic summary that can be generated from the set of input documents R. Ratings were on a 5-point scale \u2014 5 being very good and 1 being very poor. The results are summarized in Table (<>)3 showing that the majority of ratings fall under good (4) and very good (5). ",
 "3.4 Examples ",
 "Finally, we also illustrate two specific challenging aspects of the qMDS dataset. The example in Table (<>)4 demonstrates how a summary can cover multiple facets of a single query that can be sourced from multiple input documents. The example in Table (<>)5 shows how the query context may require summarization models to attend to specific portions of the source documents. ",
 "Query: characteristics of schoenberg\u2019s atonal music ",
 "Source1: Paragraph#1 is about the Musician. Paragraph#2 is about the musicians approach. Schoenberg was widely known early in his career for his success in simultaneously extending the traditionally opposed German Romantic styles of both Brahms and Wagner. Later, his name would come to personify pioneering innovations in atonality... other sources... ",
 "Summary: Schoenberg was known early in his career for simultaneously extending the traditionally opposed German Romantic styles of Brahms and Wagner. Later, his name would come to personify innovations in atonality (although Schoenberg himself detested that term) that would become the most polemical feature of 20th-century art music. In the 1920s, Schoenberg developed the twelve-tone technique, an influential compositional method of manipulating an ordered series of all twelve notes in the chromatic scale. He also coined the term developing variation and was the first modern composer to embrace ways of developing motifs without resorting to the dominance of a centralized melodic idea ",
 "Table 5: Retrieving salient information related to the query in areas deep inside a source document and not necessarily the beginning of it. ",
 "4 Experiments ",
 "Our experiments are based on running popular summarization models on both abstractive and extractive versions of our qMDS dataset. These baseline summarization experiments are categorized into two types: (i) a query-agnostic setup where the query q is ignored and the models map source documents R to long answer a as in standard SDS/MDS; and (ii) a query-based setup where the source document set R is conditionally filtered by the input query q followed by SDS/MDS approaches. ",
 "4.1 Abstractive Summarization ",
 "Hi-MAP (<>)Fabbri et al. ((<>)2019) define a hierarchical abstractive MDS model that combines a pointer-generator network ((<>)See et al., (<>)2017) with Maximal Marginal Relevance (MMR) ((<>)Carbonell and Gold-(<>)stein, (<>)1998) scores to rank sentences based on relevancy and redundancy. We used 128-d word vectors in a single layer 512-d RNN that was trained up to 10K steps with an initial learning rate of 0.15. ",
 "PEGASUS (<>)Zhang et al. ((<>)2019) propose pre-training Transformer-based MDS models with massive text corpora. Pre-training involves generating masked sentences, similar to an extractive summary. We fine-tune PEGASUS with an initial learning rate of 0.01 for 100K steps and evaluated it on our test set, with the caveat that some test CC documents were part of the pre-trained model (albeit for a different objective). ",
 "4.2 Extractive Summarization ",
 "NeuSum (<>)Zhou et al. ((<>)2018) rank sentences using scores derived from a hierarchical encoder, with top ranked sentences forming the extractive summary. While it was designed for SDS, the hierarchical document representation is well suited for adapting to the MDS setting in future work. The model used 50-d GloVe word vectors that was trained with a learning rate of 0.001 and a batch size of 32 for 50 epochs. The output was set to 4 sentences to match the long answer summary statistics. Finally, the input sequence length was 500 sentences to capture the larger size of the multi-doc input. ",
 "TextRank This is an unsupervised sentence similarity based summarization model based on weighted-graphs defined over sentences in a document ((<>)Mihalcea and Tarau, (<>)2004) that is often used as a baseline for extractive summarization. ",
 "4.3 Incorporating Query in SDS/MDS ",
 "As our dataset explicitly designed for qMDS, we modified the standard SDS/MDS setup by pre-filtering sentences from the source documents R that are relevant to query q (based on BLEU scores) as input the models. To retain source document fluency, fragments are defined at the paragraph level. Table (<>)6 and Table (<>)7 show the results with and without this variation. The filter acts as a crude attention mechanism that weeds out irrelevant content from the inputs showing improvements in all the approaches, except for PEGASUS. We believe this drop may be attributed to the sentence masking done in pre-training PEGASUS which relies on undisrupted sentence orders. ",
 "4.4 Human Evaluation ",
 "In addition to automatic evaluation, we also collected human judgements for summarization outputs of one specific abstractive MDS model (Hi-MAP) to understand the headroom available in qMDS on this dataset. We follow the question-answering approach in (<>)Clarke and Lapata ((<>)2010). ",
 "We created 32 questions from 17 randomly sampled summaries. Participants are asked to answer those questions after reading the generated summary by Hi-MAP. Their answers are scored: 1 (fully correct answer), 0.5 (partially correct answer), and 0 (incorrect answer). Note that the ground-truth answers are the answers to the ground-truth summaries. The more the participants can answer correctly from the generated summaries, ",
 "Method ",
 "R-1 ",
 "R-2 ",
 "R-L ",
 "Query-agnostic setting ",
 "Hi-MAP ",
 "28.34 ",
 "13.12 ",
 "25.15 ",
 "PEGASUS ",
 "27.08 ",
 "12.51 ",
 "22.28 ",
 "Query-based setting ",
 "Hi-MAP ",
 "30.34 ",
 "14.82 ",
 "26.86 ",
 "PEGASUS ",
 "24.61 ",
 "9.12 ",
 "19.61 ",
 "Table 6: Baselines on abstractive dataset on test split ",
 "Method ",
 "R-1 ",
 "R-2 ",
 "R-L ",
 "Query-agnostic setting ",
 "NeuSum ",
 "62.61 ",
 "54.45 ",
 "61.99 ",
 "TextRank ",
 "24.4 ",
 "15.56 ",
 "31.6 ",
 "Query-based setting ",
 "NeuSum ",
 "63.09 ",
 "55.13 ",
 "62.39 ",
 "TextRank ",
 "25.72 ",
 "17.4 ",
 "34.3 ",
 "Table 7: Baselines on extractive dataset on test split. ",
 "the better the summarization system. We then compute the averaged scores. 5 were given questions and the ground-truth summaries and the rest 5 were given the questions and the generated summaries by Hi-MAP. For ground-truth summaries, the average positive responses is 30.6 out of 32. For Hi-MAP summary, this is 13.8. This is significantly lower than the score for ground-truth showing a fairly wide headroom for improvement. ",
 "5 Related Work ",
 "Query-based summarization can be both extractive ((<>)Dang, (<>)2006; (<>)Daume\u00b4 III and Marcu, (<>)2006; (<>)Schilder and Kondadadi, (<>)2008; (<>)Otterbacher et al., (<>)2009; (<>)Wang et al., (<>)2016; (<>)Litvak and Vanetik, (<>)2017; (<>)Wang et al., (<>)2019) or abstractive ((<>)Nema et al., (<>)2017; (<>)Baumel et al., (<>)2018b; (<>)Hasselqvist et al., (<>)2017; (<>)Ishi-(<>)gaki et al., (<>)2020). Earlier studies were often extractive and relied on manually selected and curated datasets such as DUC2005 and DUC2006 ((<>)Dang, (<>)2006). However, neural abstractive models often demand large amounts of labeled data, which are hard to obtain for summarization and other tasks with similar demands on manual annotation efforts. Recent studies show a two-step process of using extractive summarization followed by generation for abstractive summaries((<>)Fabbri et al., (<>)2019) as well as for query-based abstractive summaries ((<>)Egon-(<>)mwan et al., (<>)2019). ",
 "While our work is motivated by the use case of generating longer summaries to answer com-",
 "plex questions, there are related work on creating QA datasets for short answers: using news articles from CNN/DM ((<>)Hermann et al., (<>)2015b), Hot-potQA ((<>)Yang et al., (<>)2018), TriviaQA ((<>)Joshi et al., (<>)2017), SearchQA ((<>)Dunn et al., (<>)2017), online debates with summarizing arguments and debating topics as queries ((<>)Nema et al., (<>)2017), and community question answering websites ((<>)Deng et al., (<>)2020). Some of them involve extracting text spans of words as answers from multiple text passages. However, our work focuses on longer answers. ",
 "Large-scale datasets for regular MDS over long documents with target long summaries have also started to appear ((<>)Liu et al., (<>)2018; (<>)Fabbri et al., (<>)2019; (<>)Koupaee and Wang, (<>)2018). Besides extracting contents for IR applications, our efforts differs from them in terms of heterogeneity in documents and lengths of summaries. The MS Marco dataset is close to our work in spirit ((<>)Bajaj et al., (<>)2018). The dataset contains 1M question-answer-context triplets where the answers are human created using the top-10 passages returned from Bing\u2019s search queries. We use Wikipedia passages as summaries thus avoid additional human efforts. Examining the statistics of the dataset, our dataset also has longer input sources and answers. ",
 "6 Conclusion ",
 "We have presented AQUAMUSE, a scalable methodology for constructing new qMDS datasets, along with in-depth analyses and baseline experiments to demonstrate properties of one such dataset instance. Many parts of the approach are configurable providing researchers a rich sandbox for evaluating summarization models under different task conditions. Our methodology greatly reduces the cost of data collection by converting a predominantly generative human annotation task (e.g., reading documents and writing succinct summaries) to a discriminative human annotation task (e.g., deciding on sentence-document relevance). While our present work do not propose new methods for query-based summarization, we ran baseline experiments on one specific instance of the AQUAMUSE dataset using a few popular neural approaches re-adapted with query conditioning. Our experiments demonstrates that there is still much headroom for existing state-of-the-art models and we hope AQUAMUSE will spur further advancements query focused multi-document summarization algorithms. ",
 "References ",
 "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-wary, and Tong Wang. 2018. (<http://arxiv.org/abs/1611.09268>)Ms marco: A human (<http://arxiv.org/abs/1611.09268>)generated machine reading comprehension dataset. ",
 "Tal Baumel, Matan Eyal, and Michael Elhadad. 2018a. (<http://arxiv.org/abs/1801.07704>)Query focused abstractive summarization: Incorpo-(<http://arxiv.org/abs/1801.07704>)rating query relevance, multi-document coverage, (<http://arxiv.org/abs/1801.07704>)and summary length constraints into seq2seq mod-(<http://arxiv.org/abs/1801.07704>)els. CoRR, abs/1801.07704. ",
 "Tal Baumel, Matan Eyal, and Michael Elhadad. 2018b. (<http://arxiv.org/abs/1801.07704>)Query focused abstractive summarization: Incorpo-(<http://arxiv.org/abs/1801.07704>)rating query relevance, multi-document coverage, (<http://arxiv.org/abs/1801.07704>)and summary length constraints into seq2seq mod-(<http://arxiv.org/abs/1801.07704>)els. CoRR, abs/1801.07704. ",
 "Jaime G Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In SIGIR, volume 98, pages 335\u2013336. ",
 "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. (<http://arxiv.org/abs/1803.11175>)Universal sentence encoder. CoRR, abs/1803.11175. ",
 "James Clarke and Mirella Lapata. 2010. (<https://doi.org/10.1162/coli_a_00004>)Discourse con-(<https://doi.org/10.1162/coli_a_00004>)straints for document compression. Computational Linguistics, 36(3):411\u2013441. ",
 "Hoa Trang Dang. 2006. Duc 2005: Evaluation of question-focused summarization systems. In Proceedings of the Workshop on Task-Focused Summarization and Question Answering, SumQA \u201906, page 48\u201355, USA. Association for Computational Linguistics. ",
 "Hal Daume\u00b4 III and Daniel Marcu. 2006. (<https://doi.org/10.3115/1220175.1220214>)Bayesian (<https://doi.org/10.3115/1220175.1220214>)query-focused summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 305\u2013312, Sydney, Australia. Association for Computational Linguistics. ",
 "Yang Deng, Wai Lam, Yuexiang Xie, Daoyuan Chen, Yaliang Li, Min Yang, and Ying Shen. 2020. Joint learning of answer selection and answer summary generation in community question answering. AAAI. ",
 "Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. (<http://arxiv.org/abs/1704.05179>)Searchqa: A new q&a dataset augmented with con-(<http://arxiv.org/abs/1704.05179>)text from a search engine. ",
 "Elozino Egonmwan, Vittorio Castelli, and Md Arafat Sultan. 2019. (<https://doi.org/10.18653/v1/D19-5810>)Cross-task knowledge transfer for (<https://doi.org/10.18653/v1/D19-5810>)query-based text summarization. In Proceedings of ",
 "the 2nd Workshop on Machine Reading for Question Answering, pages 72\u201377, Hong Kong, China. Association for Computational Linguistics. ",
 "Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R Radev. 2019. Multi-news: a large-scale multi-document summarization dataset and abstractive hierarchical model. ACL. ",
 "Max Grusky, Mor Naaman, and Yoav Artzi. 2018. (<http://arxiv.org/abs/1804.11283>)Newsroom: A dataset of 1.3 million sum-(<http://arxiv.org/abs/1804.11283>)maries with diverse extractive strategies. CoRR, abs/1804.11283. ",
 "Johan Hasselqvist, Niklas Helmertz, and Mikael Kageb\u00b0 ack.\u00a8 2017. (<http://arxiv.org/abs/1712.06100>)Query-based abstractive (<http://arxiv.org/abs/1712.06100>)summarization using neural networks. CoRR, abs/1712.06100. ",
 "Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015a. Teaching machines to read and comprehend. In Advances in neural infor-mation processing systems, pages 1693\u20131701. ",
 "Karl Moritz Hermann, Tomas\u00b4 Kocisky,\u00b4 Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015b. (<http://arxiv.org/abs/1506.03340>)Teaching machines to (<http://arxiv.org/abs/1506.03340>)read and comprehend. CoRR, abs/1506.03340. ",
 "Tatsuya Ishigaki, Hen-Hsen Huang, Hiroya Takamura, Hsin-Hsi Chen, and Manabu Okumura. 2020. Neu-ral query-biased abstractive summarization using copying mechanism. Advances in Information Re-trieval, 12036:174 \u2013 181. ",
 "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. (<https://doi.org/10.18653/v1/P17-1147>)TriviaQA: A large scale dis-(<https://doi.org/10.18653/v1/P17-1147>)tantly supervised challenge dataset for reading com-(<https://doi.org/10.18653/v1/P17-1147>)prehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611, Vancouver, Canada. Association for Computational Linguistics. ",
 "Mahnaz Koupaee and William Yang Wang. 2018. (<http://arxiv.org/abs/1810.09305>)Wik-(<http://arxiv.org/abs/1810.09305>)ihow: A large scale text summarization dataset. CoRR, abs/1810.09305. ",
 "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics. ",
 "Marina Litvak and Natalia Vanetik. 2017. (<https://doi.org/10.18653/v1/W17-1004>)Query-based (<https://doi.org/10.18653/v1/W17-1004>)summarization using MDL principle. In Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres, pages 22\u201331, Valencia, Spain. Association for Computational Linguistics. ",
 "Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. (<http://arxiv.org/abs/1801.10198>)Generating wikipedia by summariz-(<http://arxiv.org/abs/1801.10198>)ing long sequences. CoRR, abs/1801.10198. ",
 "Ting Liu, Andrew W. Moore, Alexander G. Gray, and Ke Yang. 2004. An investigation of practical approximate nearest neighbor algorithms. In NIPS. ",
 "Rada Mihalcea and Paul Tarau. 2004. (<https://www.aclweb.org/anthology/W04-3252>)TextRank: (<https://www.aclweb.org/anthology/W04-3252>)Bringing order into text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404\u2013411, Barcelona, Spain. Association for Computational Linguistics. ",
 "Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. Computational Natural Lan-guage Learning. ",
 "Preksha Nema, Mitesh M. Khapra, Anirban Laha, and Balaraman Ravindran. 2017. (<https://doi.org/10.18653/v1/P17-1098>)Diversity driven atten-(<https://doi.org/10.18653/v1/P17-1098>)tion model for query-based abstractive summariza-(<https://doi.org/10.18653/v1/P17-1098>)tion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 1063\u20131072. Association for Computational Linguistics. ",
 "Jahna Otterbacher, Gunes Erkan, and Dragomir R. Radev. 2009. (<https://doi.org/https://doi.org/10.1016/j.ipm.2008.06.004>)Biased LexRank: Passage retrieval us-(<https://doi.org/https://doi.org/10.1016/j.ipm.2008.06.004>)ing random walks with question-based priors. Information Processing & Management, 45(1):42 \u2013 54. ",
 "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. (<http://arxiv.org/abs/1910.10683>)Exploring the limits (<http://arxiv.org/abs/1910.10683>)of transfer learning with a unified text-to-text trans-(<http://arxiv.org/abs/1910.10683>)former. arXiv e-prints. ",
 "Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. EMNLP. ",
 "Frank Schilder and Ravikumar Kondadadi. 2008. Fast-sum: Fast and accurate query-based multi-document summarization. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short \u201908, page 205\u2013208, USA. Association for Computational Linguistics. ",
 "Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. (<http://arxiv.org/abs/1704.04368>)Get to the point: Summarization with pointer-(<http://arxiv.org/abs/1704.04368>)generator networks. CoRR, abs/1704.04368. ",
 "Hong Wang, Xin Wang, Wenhan Xiong, Mo Yu, Xi-aoxiao Guo, Shiyu Chang, and William Yang Wang. 2019. (<https://doi.org/10.18653/v1/P19-1214>)Self-supervised learning for contextualized (<https://doi.org/10.18653/v1/P19-1214>)extractive summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2221\u20132227, Florence, Italy. Association for Computational Linguistics. ",
 "Li Wang, Junlin Yao, Yunzhe Tao, Li Zhong, Wei Liu, and Qiang Du. 2018. (<http://arxiv.org/abs/1805.03616>)A reinforced topic-aware con-(<http://arxiv.org/abs/1805.03616>)volutional sequence-to-sequence model for abstrac-(<http://arxiv.org/abs/1805.03616>)tive text summarization. CoRR, abs/1805.03616. ",
 "Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Florian, and Claire Cardie. 2016. (<http://arxiv.org/abs/1606.07548>)A sentence com-(<http://arxiv.org/abs/1606.07548>)pression based framework to query-focused multi-(<http://arxiv.org/abs/1606.07548>)document summarization. CoRR, abs/1606.07548. ",
 "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. (<https://doi.org/10.18653/v1/D18-1259>)HotpotQA: A dataset (<https://doi.org/10.18653/v1/D18-1259>)for diverse, explainable multi-hop question answer-(<https://doi.org/10.18653/v1/D18-1259>)ing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics. ",
 "Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2019. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. ArXiv, abs/1912.08777. ",
 "Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. 2018. (<https://doi.org/10.18653/v1/P18-1061>)Neural doc-(<https://doi.org/10.18653/v1/P18-1061>)ument summarization by jointly learning to score (<https://doi.org/10.18653/v1/P18-1061>)and select sentences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 654\u2013 663, Melbourne, Australia. Association for Computational Linguistics. ",
